{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy.stats import sem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_localization(out_local, top_k=-0, highest=True, mean_func=np.nanmean):\n",
    "    return {\n",
    "        noise_level: {\n",
    "            noise_features: {\n",
    "                method: {\n",
    "                    \"mean\": np.round(mean_func(instance_localizations[-(top_k):] if highest else instance_localizations[:top_k]), 2),\n",
    "                    \"standard_error\": np.round(np.std(instance_localizations[-(top_k):] if highest else instance_localizations[:top_k], ddof=1) / np.sqrt(np.size(instance_localizations[-(top_k):] if highest else instance_localizations[:top_k])), 3),\n",
    "                    \"se_2\": np.round(sem(instance_localizations[-(top_k):] if highest else instance_localizations[:top_k], nan_policy='omit'),3)\n",
    "                    }\n",
    "                for method, instance_localizations in noise_features_dict.items()\n",
    "            }\n",
    "            for noise_features, noise_features_dict in data.items()\n",
    "        }\n",
    "        for noise_level, data in out_local.items()\n",
    "    }    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fixed_synthetic_out_localization_simple_1.json', 'r') as f:\n",
    "  syn_simple_out_localization_1 = json.load(f)\n",
    "with open('fixed_synthetic_out_localization_simple_2.json', 'r') as f:\n",
    "  syn_simple_out_localization_2 = json.load(f)\n",
    "with open('fixed_synthetic_out_localization_simple_3.json', 'r') as f:\n",
    "  syn_simple_out_localization_3 = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank {'varx_ig': {'mean': 0.78, 'standard_error': 0.006, 'se_2': 0.006}, 'varx_lrp': {'mean': 0.75, 'standard_error': 0.006, 'se_2': 0.006}, 'varx': {'mean': 0.88, 'standard_error': 0.004, 'se_2': 0.004}, 'clue': {'mean': 0.07, 'standard_error': 0.003, 'se_2': 0.003}, 'infoshap': {'mean': 0.59, 'standard_error': 0.005, 'se_2': 0.005}}\n",
      "Mass {'varx_ig': {'mean': 0.53, 'standard_error': 0.004, 'se_2': 0.004}, 'varx_lrp': {'mean': 0.5, 'standard_error': 0.004, 'se_2': 0.004}, 'varx': {'mean': 0.79, 'standard_error': 0.003, 'se_2': 0.003}, 'clue': {'mean': 0.07, 'standard_error': 0.001, 'se_2': 0.001}, 'infoshap': {'mean': 0.31, 'standard_error': 0.003, 'se_2': 0.003}}\n"
     ]
    }
   ],
   "source": [
    "s_rank_aggr_1 = aggregate_localization(syn_simple_out_localization_1[\"local_localization_precision\"], top_k=-0, highest=True)\n",
    "print(\"Rank\", s_rank_aggr_1[\"2\"][\"5\"])\n",
    "\n",
    "s_mass_aggr_1 = aggregate_localization(syn_simple_out_localization_1[\"local_localization_mass_accuracy\"], top_k=-0, highest=True)\n",
    "print(\"Mass\", s_mass_aggr_1[\"2\"][\"5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank {'varx_ig': {'mean': 0.75, 'standard_error': 0.006, 'se_2': 0.006}, 'varx_lrp': {'mean': 0.74, 'standard_error': 0.006, 'se_2': 0.006}, 'varx': {'mean': 0.86, 'standard_error': 0.004, 'se_2': 0.004}, 'clue': {'mean': 0.06, 'standard_error': 0.003, 'se_2': 0.003}, 'infoshap': {'mean': 0.59, 'standard_error': 0.006, 'se_2': 0.006}}\n",
      "Mass {'varx_ig': {'mean': 0.5, 'standard_error': 0.004, 'se_2': 0.004}, 'varx_lrp': {'mean': 0.48, 'standard_error': 0.004, 'se_2': 0.004}, 'varx': {'mean': 0.77, 'standard_error': 0.003, 'se_2': 0.003}, 'clue': {'mean': 0.07, 'standard_error': 0.001, 'se_2': 0.001}, 'infoshap': {'mean': 0.31, 'standard_error': 0.003, 'se_2': 0.003}}\n"
     ]
    }
   ],
   "source": [
    "s_rank_aggr_2 = aggregate_localization(syn_simple_out_localization_2[\"local_localization_precision\"], top_k=-0, highest=True)\n",
    "print(\"Rank\", s_rank_aggr_2[\"2\"][\"5\"])\n",
    "\n",
    "s_mass_aggr_2 = aggregate_localization(syn_simple_out_localization_2[\"local_localization_mass_accuracy\"], top_k=-0, highest=True)\n",
    "print(\"Mass\", s_mass_aggr_2[\"2\"][\"5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank {'varx_ig': {'mean': 0.78, 'standard_error': 0.006, 'se_2': 0.006}, 'varx_lrp': {'mean': 0.75, 'standard_error': 0.006, 'se_2': 0.006}, 'varx': {'mean': 0.86, 'standard_error': 0.004, 'se_2': 0.004}, 'clue': {'mean': 0.06, 'standard_error': 0.003, 'se_2': 0.003}, 'infoshap': {'mean': 0.6, 'standard_error': 0.006, 'se_2': 0.006}}\n",
      "Mass {'varx_ig': {'mean': 0.52, 'standard_error': 0.004, 'se_2': 0.004}, 'varx_lrp': {'mean': 0.49, 'standard_error': 0.004, 'se_2': 0.004}, 'varx': {'mean': 0.77, 'standard_error': 0.003, 'se_2': 0.003}, 'clue': {'mean': 0.07, 'standard_error': 0.001, 'se_2': 0.001}, 'infoshap': {'mean': 0.32, 'standard_error': 0.003, 'se_2': 0.003}}\n"
     ]
    }
   ],
   "source": [
    "s_rank_aggr_3 = aggregate_localization(syn_simple_out_localization_3[\"local_localization_precision\"], top_k=-0, highest=True)\n",
    "print(\"Rank\", s_rank_aggr_3[\"2\"][\"5\"])\n",
    "\n",
    "s_mass_aggr_3 = aggregate_localization(syn_simple_out_localization_3[\"local_localization_mass_accuracy\"], top_k=-0, highest=True)\n",
    "print(\"Mass\", s_mass_aggr_3[\"2\"][\"5\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fixed_synthetic_out_localization_1.json', 'r') as f:\n",
    "  syn_out_localization_1 = json.load(f)\n",
    "with open('fixed_synthetic_out_localization_2.json', 'r') as f:\n",
    "  syn_out_localization_2 = json.load(f)\n",
    "with open('fixed_synthetic_out_localization_3.json', 'r') as f:\n",
    "  syn_out_localization_3 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank {'varx_ig': {'mean': 0.39, 'standard_error': 0.006, 'se_2': 0.006}, 'varx_lrp': {'mean': 0.43, 'standard_error': 0.006, 'se_2': 0.006}, 'varx': {'mean': 0.73, 'standard_error': 0.005, 'se_2': 0.005}, 'clue': {'mean': 0.06, 'standard_error': 0.003, 'se_2': 0.003}, 'infoshap': {'mean': 0.56, 'standard_error': 0.005, 'se_2': 0.005}}\n",
      "Mass {'varx_ig': {'mean': 0.25, 'standard_error': 0.004, 'se_2': 0.004}, 'varx_lrp': {'mean': 0.28, 'standard_error': 0.003, 'se_2': 0.003}, 'varx': {'mean': 0.46, 'standard_error': 0.003, 'se_2': 0.003}, 'clue': {'mean': 0.07, 'standard_error': 0.001, 'se_2': 0.001}, 'infoshap': {'mean': 0.28, 'standard_error': 0.003, 'se_2': 0.003}}\n"
     ]
    }
   ],
   "source": [
    "c_rank_aggr_1 = aggregate_localization(syn_out_localization_1[\"local_localization_precision\"], top_k=-0, highest=True)\n",
    "print(\"Rank\", c_rank_aggr_1[\"2\"][\"5\"])\n",
    "\n",
    "c_mass_aggr_1 = aggregate_localization(syn_out_localization_1[\"local_localization_mass_accuracy\"], top_k=-0, highest=True)\n",
    "print(\"Mass\", c_mass_aggr_1[\"2\"][\"5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank {'varx_ig': {'mean': 0.39, 'standard_error': 0.005, 'se_2': 0.005}, 'varx_lrp': {'mean': 0.37, 'standard_error': 0.005, 'se_2': 0.005}, 'varx': {'mean': 0.59, 'standard_error': 0.005, 'se_2': 0.005}, 'clue': {'mean': 0.07, 'standard_error': 0.003, 'se_2': 0.003}, 'infoshap': {'mean': 0.44, 'standard_error': 0.005, 'se_2': 0.005}}\n",
      "Mass {'varx_ig': {'mean': 0.23, 'standard_error': 0.003, 'se_2': 0.003}, 'varx_lrp': {'mean': 0.23, 'standard_error': 0.003, 'se_2': 0.003}, 'varx': {'mean': 0.33, 'standard_error': 0.003, 'se_2': 0.003}, 'clue': {'mean': 0.07, 'standard_error': 0.001, 'se_2': 0.001}, 'infoshap': {'mean': 0.23, 'standard_error': 0.002, 'se_2': 0.002}}\n"
     ]
    }
   ],
   "source": [
    "c_rank_aggr_2 = aggregate_localization(syn_out_localization_2[\"local_localization_precision\"], top_k=-0, highest=True)\n",
    "print(\"Rank\", c_rank_aggr_2[\"2\"][\"5\"])\n",
    "\n",
    "c_mass_aggr_2 = aggregate_localization(syn_out_localization_2[\"local_localization_mass_accuracy\"], top_k=-0, highest=True)\n",
    "print(\"Mass\", c_mass_aggr_2[\"2\"][\"5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank {'varx_ig': {'mean': 0.36, 'standard_error': 0.006, 'se_2': 0.006}, 'varx_lrp': {'mean': 0.38, 'standard_error': 0.005, 'se_2': 0.005}, 'varx': {'mean': 0.64, 'standard_error': 0.005, 'se_2': 0.005}, 'clue': {'mean': 0.06, 'standard_error': 0.003, 'se_2': 0.003}, 'infoshap': {'mean': 0.5, 'standard_error': 0.005, 'se_2': 0.005}}\n",
      "Mass {'varx_ig': {'mean': 0.24, 'standard_error': 0.003, 'se_2': 0.003}, 'varx_lrp': {'mean': 0.25, 'standard_error': 0.003, 'se_2': 0.003}, 'varx': {'mean': 0.4, 'standard_error': 0.003, 'se_2': 0.003}, 'clue': {'mean': 0.07, 'standard_error': 0.001, 'se_2': 0.001}, 'infoshap': {'mean': 0.26, 'standard_error': 0.002, 'se_2': 0.002}}\n"
     ]
    }
   ],
   "source": [
    "c_rank_aggr_3 = aggregate_localization(syn_out_localization_3[\"local_localization_precision\"], top_k=-0, highest=True)\n",
    "print(\"Rank\", c_rank_aggr_3[\"2\"][\"5\"])\n",
    "\n",
    "c_mass_aggr_3 = aggregate_localization(syn_out_localization_3[\"local_localization_mass_accuracy\"], top_k=-0, highest=True)\n",
    "print(\"Mass\", c_mass_aggr_3[\"2\"][\"5\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red Wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fixed_red_wine_localization_simple.json', 'r') as f:\n",
    "    rw_simple_out_localization_1 = json.load(f)\n",
    "with open('fixed_red_wine_50_out_localization.json', 'r') as f:\n",
    "    rw_out_localization_50 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank {'varx_ig': {'mean': 0.66, 'standard_error': 0.01, 'se_2': 0.01}, 'varx_lrp': {'mean': 0.66, 'standard_error': 0.011, 'se_2': 0.011}, 'varx': {'mean': 0.89, 'standard_error': 0.007, 'se_2': 0.007}, 'clue': {'mean': 0.58, 'standard_error': 0.01, 'se_2': 0.01}, 'infoshap': {'mean': 0.37, 'standard_error': 0.01, 'se_2': 0.01}}\n",
      "Mass {'varx_ig': {'mean': 0.62, 'standard_error': 0.009, 'se_2': 0.009}, 'varx_lrp': {'mean': 0.6, 'standard_error': 0.01, 'se_2': 0.01}, 'varx': {'mean': 0.9, 'standard_error': 0.004, 'se_2': 0.004}, 'clue': {'mean': 0.52, 'standard_error': 0.008, 'se_2': 0.008}, 'infoshap': {'mean': 0.35, 'standard_error': 0.006, 'se_2': 0.006}}\n"
     ]
    }
   ],
   "source": [
    "rw_rank_aggr_simple = aggregate_localization(rw_simple_out_localization_1[\"local_localization_precision\"], top_k=-0, highest=True)\n",
    "print(\"Rank\", rw_rank_aggr_simple[\"2\"][\"5\"])\n",
    "\n",
    "rw_mass_aggr_simple = aggregate_localization(rw_simple_out_localization_1[\"local_localization_mass_accuracy\"], top_k=-0, highest=True)\n",
    "print(\"Mass\", rw_mass_aggr_simple[\"2\"][\"5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank {'varx_ig': {'mean': 0.67, 'standard_error': 0.01, 'se_2': 0.01}, 'varx_lrp': {'mean': 0.67, 'standard_error': 0.01, 'se_2': 0.01}, 'varx': {'mean': 0.93, 'standard_error': 0.006, 'se_2': 0.006}, 'clue': {'mean': 0.66, 'standard_error': 0.009, 'se_2': 0.009}, 'infoshap': {'mean': 0.77, 'standard_error': 0.009, 'se_2': 0.009}}\n",
      "Mass {'varx_ig': {'mean': 0.71, 'standard_error': 0.01, 'se_2': 0.01}, 'varx_lrp': {'mean': 0.71, 'standard_error': 0.009, 'se_2': 0.009}, 'varx': {'mean': 0.94, 'standard_error': 0.002, 'se_2': 0.002}, 'clue': {'mean': 0.62, 'standard_error': 0.007, 'se_2': 0.007}, 'infoshap': {'mean': 0.72, 'standard_error': 0.006, 'se_2': 0.006}}\n"
     ]
    }
   ],
   "source": [
    "rw_rank_aggr_50 = aggregate_localization(rw_out_localization_50[\"local_localization_precision\"], top_k=-0, highest=True)\n",
    "print(\"Rank\", rw_rank_aggr_50[\"2\"][\"5\"])\n",
    "\n",
    "rw_mass_aggr_50 = aggregate_localization(rw_out_localization_50[\"local_localization_mass_accuracy\"], top_k=-0, highest=True)\n",
    "print(\"Mass\", rw_mass_aggr_50[\"2\"][\"5\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ailerons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fixed_ailerons_localization_simple.json', 'r') as f:\n",
    "    a_simple_out_localization_1 = json.load(f)\n",
    "with open('fixed_ailerons_50_out_localization.json', 'r') as f:\n",
    "    a_out_localization_50 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank {'varx_ig': {'mean': 0.81, 'standard_error': 0.003, 'se_2': 0.003}, 'varx_lrp': {'mean': 0.8, 'standard_error': 0.003, 'se_2': 0.003}, 'varx': {'mean': 0.86, 'standard_error': 0.003, 'se_2': 0.003}, 'clue': {'mean': 0.52, 'standard_error': 0.003, 'se_2': 0.003}, 'infoshap': {'mean': 0.51, 'standard_error': 0.004, 'se_2': 0.004}}\n",
      "Mass {'varx_ig': {'mean': 0.78, 'standard_error': 0.003, 'se_2': 0.003}, 'varx_lrp': {'mean': 0.77, 'standard_error': 0.003, 'se_2': 0.003}, 'varx': {'mean': 0.86, 'standard_error': 0.002, 'se_2': 0.002}, 'clue': {'mean': 0.36, 'standard_error': 0.002, 'se_2': 0.002}, 'infoshap': {'mean': 0.39, 'standard_error': 0.002, 'se_2': 0.002}}\n"
     ]
    }
   ],
   "source": [
    "a_rank_aggr_simple = aggregate_localization(a_simple_out_localization_1[\"local_localization_precision\"], top_k=-0, highest=True)\n",
    "print(\"Rank\", a_rank_aggr_simple[\"2\"][\"5\"])\n",
    "\n",
    "a_mass_aggr_simple = aggregate_localization(a_simple_out_localization_1[\"local_localization_mass_accuracy\"], top_k=-0, highest=True)\n",
    "print(\"Mass\", a_mass_aggr_simple[\"2\"][\"5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank {'varx_ig': {'mean': 0.74, 'standard_error': 0.003, 'se_2': 0.003}, 'varx_lrp': {'mean': 0.69, 'standard_error': 0.005, 'se_2': 0.005}, 'varx': {'mean': 0.9, 'standard_error': 0.002, 'se_2': 0.002}, 'clue': {'mean': 0.6, 'standard_error': 0.003, 'se_2': 0.003}, 'infoshap': {'mean': 0.93, 'standard_error': 0.002, 'se_2': 0.002}}\n",
      "Mass {'varx_ig': {'mean': 0.79, 'standard_error': 0.004, 'se_2': 0.004}, 'varx_lrp': {'mean': 0.76, 'standard_error': nan, 'se_2': 0.005}, 'varx': {'mean': 0.9, 'standard_error': 0.001, 'se_2': 0.001}, 'clue': {'mean': 0.48, 'standard_error': 0.002, 'se_2': 0.002}, 'infoshap': {'mean': 0.86, 'standard_error': 0.001, 'se_2': 0.001}}\n"
     ]
    }
   ],
   "source": [
    "a_rank_aggr_50 = aggregate_localization(a_out_localization_50[\"local_localization_precision\"], top_k=-0, highest=True)\n",
    "print(\"Rank\", a_rank_aggr_50[\"2\"][\"5\"])\n",
    "\n",
    "a_mass_aggr_50 = aggregate_localization(a_out_localization_50[\"local_localization_mass_accuracy\"], top_k=-0, highest=True)\n",
    "print(\"Mass\", a_mass_aggr_50[\"2\"][\"5\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fixed_lsat_localization_simple.json', 'r') as f:\n",
    "    lsat_simple_out_localization_1 = json.load(f)\n",
    "with open('fixed_lsat_50_out_localization.json', 'r') as f:\n",
    "    lsat_out_localization_50 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank {'varx_ig': {'mean': 0.79, 'standard_error': 0.002, 'se_2': 0.002}, 'varx_lrp': {'mean': 0.78, 'standard_error': 0.002, 'se_2': 0.002}, 'varx': {'mean': 0.92, 'standard_error': 0.002, 'se_2': 0.002}, 'clue': {'mean': 0.5, 'standard_error': 0.002, 'se_2': 0.002}, 'infoshap': {'mean': 0.78, 'standard_error': 0.002, 'se_2': 0.002}}\n",
      "Mass {'varx_ig': {'mean': 0.8, 'standard_error': 0.002, 'se_2': 0.002}, 'varx_lrp': {'mean': 0.79, 'standard_error': 0.002, 'se_2': 0.002}, 'varx': {'mean': 0.94, 'standard_error': 0.001, 'se_2': 0.001}, 'clue': {'mean': 0.49, 'standard_error': nan, 'se_2': 0.002}, 'infoshap': {'mean': 0.78, 'standard_error': 0.002, 'se_2': 0.002}}\n"
     ]
    }
   ],
   "source": [
    "lsat_rank_aggr_simple = aggregate_localization(lsat_simple_out_localization_1[\"local_localization_precision\"], top_k=-0, highest=True)\n",
    "print(\"Rank\", lsat_rank_aggr_simple[\"2\"][\"5\"])\n",
    "\n",
    "lsat_mass_aggr_simple = aggregate_localization(lsat_simple_out_localization_1[\"local_localization_mass_accuracy\"], top_k=-0, highest=True)\n",
    "print(\"Mass\", lsat_mass_aggr_simple[\"2\"][\"5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank {'varx_ig': {'mean': 0.81, 'standard_error': 0.002, 'se_2': 0.002}, 'varx_lrp': {'mean': 0.74, 'standard_error': 0.003, 'se_2': 0.003}, 'varx': {'mean': 0.95, 'standard_error': 0.001, 'se_2': 0.001}, 'clue': {'mean': 0.49, 'standard_error': 0.002, 'se_2': 0.002}, 'infoshap': {'mean': 0.95, 'standard_error': 0.001, 'se_2': 0.001}}\n",
      "Mass {'varx_ig': {'mean': 0.89, 'standard_error': 0.001, 'se_2': 0.001}, 'varx_lrp': {'mean': 0.82, 'standard_error': nan, 'se_2': 0.003}, 'varx': {'mean': 0.96, 'standard_error': 0.001, 'se_2': 0.001}, 'clue': {'mean': 0.52, 'standard_error': 0.002, 'se_2': 0.002}, 'infoshap': {'mean': 0.96, 'standard_error': 0.001, 'se_2': 0.001}}\n"
     ]
    }
   ],
   "source": [
    "lsat_rank_aggr_50 = aggregate_localization(lsat_out_localization_50[\"local_localization_precision\"], top_k=-0, highest=True)\n",
    "print(\"Rank\", lsat_rank_aggr_50[\"2\"][\"5\"])\n",
    "\n",
    "lsat_mass_aggr_50 = aggregate_localization(lsat_out_localization_50[\"local_localization_mass_accuracy\"], top_k=-0, highest=True)\n",
    "print(\"Mass\", lsat_mass_aggr_50[\"2\"][\"5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def create_latex_table():\n",
    "    # Function to format mean ± std as string\n",
    "    def format_metric(mean, se):\n",
    "        return f\"{mean:.2f} ± {se:.2f}\"\n",
    "\n",
    "    # Initialize the LaTeX table content\n",
    "    latex_content = [\n",
    "        \"\\\\begin{table}[h]\",\n",
    "        \"\\\\centering\",\n",
    "        \"\\\\caption{Localization Results}\",\n",
    "        \"\\\\label{tab:localization}\",\n",
    "        \"\\\\begin{tabular}{lcccccc}\",\n",
    "        \"\\\\toprule\",\n",
    "        \"Dataset & Metric & VarX-IG & VarX-LRP & VarX & CLUE & InfoShap \\\\\\\\\",\n",
    "        \"\\\\midrule\"\n",
    "    ]\n",
    "\n",
    "    # Load all datasets\n",
    "    datasets = {\n",
    "        \"Synthetic (Simple)\": [\n",
    "            (\"fixed_synthetic_out_localization_simple_1.json\", \"syn_simple_out_localization_1\"),\n",
    "            (\"fixed_synthetic_out_localization_simple_2.json\", \"syn_simple_out_localization_2\"),\n",
    "            (\"fixed_synthetic_out_localization_simple_3.json\", \"syn_simple_out_localization_3\")\n",
    "        ],\n",
    "        \"Synthetic (Complex)\": [\n",
    "            (\"fixed_synthetic_out_localization_1.json\", \"syn_out_localization_1\"),\n",
    "            (\"fixed_synthetic_out_localization_2.json\", \"syn_out_localization_2\"),\n",
    "            (\"fixed_synthetic_out_localization_3.json\", \"syn_out_localization_3\")\n",
    "        ],\n",
    "        \"Red Wine\": [\n",
    "            (\"fixed_red_wine_localization_simple.json\", \"rw_simple_out_localization_1\"),\n",
    "            (\"fixed_red_wine_50_out_localization.json\", \"rw_out_localization_50\")\n",
    "        ],\n",
    "        \"Ailerons\": [\n",
    "            (\"fixed_ailerons_localization_simple.json\", \"a_simple_out_localization_1\"),\n",
    "            (\"fixed_ailerons_50_out_localization.json\", \"a_out_localization_50\")\n",
    "        ],\n",
    "        \"LSAT\": [\n",
    "            (\"fixed_lsat_localization_simple.json\", \"lsat_simple_out_localization_1\"),\n",
    "            (\"fixed_lsat_50_out_localization.json\", \"lsat_out_localization_50\")\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "    # Process each dataset\n",
    "    for dataset_name, files in datasets.items():\n",
    "        # Initialize arrays to store results across runs\n",
    "        rank_results = []\n",
    "        mass_results = []\n",
    "        \n",
    "        # Process each file for the dataset\n",
    "        for filename, _ in files:\n",
    "            try:\n",
    "                with open(filename, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                rank_aggr = aggregate_localization(data[\"local_localization_precision\"])\n",
    "                mass_aggr = aggregate_localization(data[\"local_localization_mass_accuracy\"])\n",
    "                \n",
    "                rank_results.append(rank_aggr[\"2\"][\"5\"])\n",
    "                mass_results.append(mass_aggr[\"2\"][\"5\"])\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Warning: File {filename} not found\")\n",
    "                continue\n",
    "\n",
    "        # Average results across runs if we have multiple runs\n",
    "        if rank_results and mass_results:\n",
    "            # Process rank results\n",
    "            methods = [\"varx_ig\", \"varx_lrp\", \"varx\", \"clue\", \"infoshap\"]\n",
    "            rank_means = {method: np.mean([run[method][\"mean\"] for run in rank_results]) for method in methods}\n",
    "            rank_ses = {method: np.mean([run[method][\"standard_error\"] for run in rank_results]) for method in methods}\n",
    "            \n",
    "            # Process mass results\n",
    "            mass_means = {method: np.mean([run[method][\"mean\"] for run in mass_results]) for method in methods}\n",
    "            mass_ses = {method: np.mean([run[method][\"standard_error\"] for run in mass_results]) for method in methods}\n",
    "\n",
    "            # Add to LaTeX table\n",
    "            latex_content.extend([\n",
    "                f\"{dataset_name} & Rank & {format_metric(rank_means['varx_ig'], rank_ses['varx_ig'])} & {format_metric(rank_means['varx_lrp'], rank_ses['varx_lrp'])} & {format_metric(rank_means['varx'], rank_ses['varx'])} & {format_metric(rank_means['clue'], rank_ses['clue'])} & {format_metric(rank_means['infoshap'], rank_ses['infoshap'])} \\\\\\\\\",\n",
    "                f\"& Mass & {format_metric(mass_means['varx_ig'], mass_ses['varx_ig'])} & {format_metric(mass_means['varx_lrp'], mass_ses['varx_lrp'])} & {format_metric(mass_means['varx'], mass_ses['varx'])} & {format_metric(mass_means['clue'], mass_ses['clue'])} & {format_metric(mass_means['infoshap'], mass_ses['infoshap'])} \\\\\\\\\"\n",
    "            ])\n",
    "\n",
    "    # Close the table\n",
    "    latex_content.extend([\n",
    "        \"\\\\bottomrule\",\n",
    "        \"\\\\end{tabular}\",\n",
    "        \"\\\\end{table}\"\n",
    "    ])\n",
    "\n",
    "    # Write to file\n",
    "    with open('localization_results.tex', 'w') as f:\n",
    "        f.write('\\n'.join(latex_content))\n",
    "\n",
    "create_latex_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(Path(\".\").glob(\"*lipschitz*.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load json with all resulst and just extract the Lipschitz metric for each method\n",
    "for file_path in Path(\".\").glob(\"*lipschitz*.json\"):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lipschitz = json.load(f)\n",
    "    pd.DataFrame({\n",
    "        method: data[\"L_out\"]\n",
    "        for method, data in lipschitz.items()\n",
    "    }).to_csv(f\"../../plotting/data/lipschitz/{file_path.stem}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clue",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
